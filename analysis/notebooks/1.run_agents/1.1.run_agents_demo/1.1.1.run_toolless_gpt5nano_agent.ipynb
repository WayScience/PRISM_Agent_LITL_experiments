{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool-less Agent Demo\n",
    "\n",
    "This script demonstrates a minimal agentic AI experiment using:\n",
    "- GPT5-nano model\n",
    "- `HUCCT1_BILIARY_TRACT` cell line from the DepMap PRISM IC50 dataset\n",
    "\n",
    "### Overview\n",
    "- **Data Source**: Preprocessed PRISM secondary drug repurposing dataset (`processed_depmap_prism_ic50.csv`).  \n",
    "- **Lookup & Dispatch**:  \n",
    "  - `PrismLookup` retrieves IC50 values by drug and cell line identifiers.  \n",
    "  - `PrismDispatchQueue` sequentially dispatches drug–cell line prediction tasks with optional shuffling.  \n",
    "- **Agent**:  \n",
    "  - A `dspy.Predict` agent configured with `PredictIC50DrugCell` signature.  \n",
    "  - Operates without external tools (tool-less mode).  \n",
    "  - Only makes predictions on the first 100 drugs and uses the smaller `openai/gpt-5-nano` model to reduce API usage.\n",
    "- **Logging**:  \n",
    "  - Each dispatched prediction is wrapped in a `TraceUnit`.  \n",
    "  - Results (IC50 prediction, confidence, explanation, trajectory) are logged to JSONL under `analysis/log/toolless/<CCLE_NAME>/`.\n",
    "\n",
    "### Purpose\n",
    "This demo provides an example with agentic prediction pipelines in a **lab-in-the-loop (LITL)** context. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in IPython shell\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import dspy\n",
    "from openai import OpenAI\n",
    "\n",
    "from dspy_litl_agentic_system.tasks.prism_lookup import PrismLookup\n",
    "from dspy_litl_agentic_system.tasks.task_dispatcher import PrismDispatchQueue\n",
    "from dspy_litl_agentic_system.agent.signatures import PredictIC50DrugCell\n",
    "from dspy_litl_agentic_system.agent.trace_unit import TraceUnit\n",
    "from dspy_litl_agentic_system.utils.jsonl_log import append_jsonl\n",
    "from nbutils.pathing import project_file, repo_root\n",
    "from nbutils.utils import IN_NOTEBOOK\n",
    "\n",
    "if IN_NOTEBOOK:\n",
    "    print(\"Running in IPython shell\")\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    print(\"Running in standard Python shell\")\n",
    "    from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100 # for low API usage during demo\n",
    "EXPERIMENTAL_DESCRIPTION = \"\"\"\n",
    "    The chemcial-peturbation viability screen is conducted in a 8-step, \n",
    "    4-fold dilution, starting from 10μM.\n",
    "    \"\"\" # Adapted from PRISM description\n",
    "\n",
    "UNIT = 'uM' # PRISM standard unit\n",
    "\n",
    "CCLE_NAME = \"HUCCT1_BILIARY_TRACT\" # an arbitrary cell line for demo\n",
    "SHUFFLE_QUEUE = True\n",
    "SHUFFLE_SEED = 42\n",
    "LM_CONFIG = {\n",
    "    \"model\": \"openai/gpt-5-nano\", # small model for demo\n",
    "    # Controls the randomness of the model's output (add variability to \n",
    "    # next token sampling). Lower values make the output more\n",
    "    # deterministic, while higher values increase randomness.\n",
    "    # A value of 1.0 is used here \n",
    "    # for the demo, balancing creativity and determinism.\n",
    "    \"temperature\": 1.0, \n",
    "    \"max_tokens\": 20000,\n",
    "    \"seed\": 42\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processed Data and Logging path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_root = repo_root()\n",
    "\n",
    "all_data_path = Path(git_root) \\\n",
    "    / \"data\" / \"processed\" / \"processed_depmap_prism_ic50.csv\"\n",
    "prism_all_data = pd.read_csv(all_data_path)\n",
    "\n",
    "log_path = Path(git_root) \\\n",
    "    / \"analysis\" / \"log\" / \"demo\" / \"toolless\" / CCLE_NAME\n",
    "log_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "representation = f\"step={N};ccle={CCLE_NAME};shuffle={SHUFFLE_QUEUE};\" \\\n",
    "                 f\"seed={SHUFFLE_SEED};\"\n",
    "representation += ';'.join(f\"{key}={val}\" \\\n",
    "                           for key, val in LM_CONFIG.items()).replace('/', '_')\n",
    "\n",
    "log_file = log_path / f\"{representation}.jsonl\"\n",
    "if log_file.exists():\n",
    "    log_file.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Global Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Env Var Key (masked)                Status\n",
      "Service                                                   \n",
      "openai   OPENAI_API_KEY     ********  Exported & Validated\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Locate config file ---\n",
    "try:\n",
    "    config_path = project_file(\"config.yml\")\n",
    "except NameError:\n",
    "    config_path = Path(git_root) / \"config.yml\"\n",
    "\n",
    "if not config_path.exists():\n",
    "    raise FileNotFoundError(f\"Config file not found at: {config_path}\")\n",
    "\n",
    "# --- Step 2: Load config.yml ---\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f) or {}\n",
    "\n",
    "# --- Step 3: Validate api section ---\n",
    "api_cfg = config.get(\"api\")\n",
    "if not api_cfg:\n",
    "    raise ValueError(\"Missing 'api' section in config.yml\")\n",
    "\n",
    "# Required keys\n",
    "required = {\"openai\": \"OPENAI_API_KEY\"}\n",
    "\n",
    "# --- Step 4: Collect resolved paths ---\n",
    "rows, errors = [], []\n",
    "for service, env_var in required.items():\n",
    "    svc_cfg = api_cfg.get(service)\n",
    "\n",
    "    if svc_cfg is None:\n",
    "        rows.append((service, env_var, None, \"Missing in config\"))\n",
    "        errors.append(f\"Config for '{service}' is missing under 'api'\")\n",
    "        continue\n",
    "\n",
    "    # Expect a structure like: api: { openai: { key: \"...\"} }\n",
    "    key = svc_cfg.get(\"key\")\n",
    "    if not isinstance(key, str) or not key.strip():\n",
    "        rows.append((service, env_var, None, \"Missing 'key' or empty\"))\n",
    "        errors.append(f\"Missing or empty 'key' for '{service}' in config.yml\")\n",
    "        continue\n",
    "\n",
    "    # Set environment variable\n",
    "    os.environ[env_var] = key\n",
    "    status_str = \"\" \n",
    "    if os.getenv(env_var):\n",
    "        status_str += \"Exported\"\n",
    "        try:\n",
    "            # use list models to validate key\n",
    "            # avoids calling a model that may incur cost\n",
    "            client = OpenAI(api_key=key)\n",
    "            models = client.models.list()\n",
    "        except Exception as e:\n",
    "            status_str += f\"& Error: {str(e)}\"\n",
    "        status_str += \" & Validated\"\n",
    "    else:\n",
    "        status_str += \" Failed to export\"\n",
    "\n",
    "    rows.append((service, env_var, \"********\", status_str))\n",
    "\n",
    "# --- Step 5: Display summary nicely ---\n",
    "config_df = (\n",
    "    pd.DataFrame(rows, columns=[\"Service\", \"Env Var\", \"Key (masked)\", \"Status\"])\n",
    "      .set_index(\"Service\")\n",
    ")\n",
    "print(config_df)\n",
    "\n",
    "# --- Step 6: Fail if any errors were collected ---\n",
    "if errors:\n",
    "    raise ValueError(\n",
    "        \"API config validation failed:\\n\" +\n",
    "        \"\\n\".join(f\"- {e}\" for e in errors) +\n",
    "        \"\\nPlease refer to /config.yml.template for correct specification.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize LooUp and Dispatcher class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset size: 778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup = PrismLookup(\n",
    "    prism_all_data,\n",
    "    drug_col=\"name\",\n",
    "    cell_col=\"ccle_name\",\n",
    "    ic50_col=\"ic50\",\n",
    "    casefold=False,\n",
    "    validate_unique=True,\n",
    ")\n",
    "subset_lookup = lookup.subset(f\"ccle_name == '{CCLE_NAME}'\")\n",
    "print(f\"Subset size: {len(subset_lookup)}\")\n",
    "\n",
    "dispatcher = PrismDispatchQueue(\n",
    "    subset_lookup,\n",
    "    shuffle=True, # order of drugs is shuffled\n",
    "    seed=42 # but reproducibile\n",
    ")\n",
    "dispatcher.has_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize LM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.configure(\n",
    "    lm=dspy.LM(\n",
    "        **LM_CONFIG\n",
    "    ))\n",
    "agent = dspy.Predict(\n",
    "    PredictIC50DrugCell,\n",
    "    tools=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop across the subset of drugs and predict IC50 values\n",
    "The run will be very quick as DSPy caches the lm output, \n",
    "when running for the first time this can take 10-20 mintues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659eec00c5cc4e3da5dead7e9e491a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Agentic Predictions over Queue of Drugs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_n = min(N, dispatcher.remaining)\n",
    "\n",
    "trace = OrderedDict()\n",
    "\n",
    "for i in tqdm(\n",
    "    range(_n), \n",
    "    desc=\"Running Agentic Predictions over Queue of Drugs\",\n",
    "    total=_n\n",
    "    ):\n",
    "\n",
    "    dispatch_item = dispatcher.dispatch()\n",
    "    trace_unit = TraceUnit(\n",
    "        drug=dispatch_item.drug,\n",
    "        cell_line=dispatch_item.cell,\n",
    "        experimental_description=EXPERIMENTAL_DESCRIPTION,\n",
    "        output_unit=UNIT,\n",
    "        ic50_true=dispatch_item.ic50\n",
    "    )\n",
    "    \n",
    "    result = agent(\n",
    "        drug=trace_unit.drug,\n",
    "        cell_line=trace_unit.cell_line,\n",
    "        experimental_description=trace_unit.experimental_description,\n",
    "        output_unit=trace_unit.output_unit\n",
    "    )\n",
    "\n",
    "    trace_unit.ic50_pred = result.ic50_pred\n",
    "    trace_unit.confidence = result.confidence\n",
    "    trace_unit.explanation = result.explanation\n",
    "    trace_unit.trajecory = result.trajectory if hasattr(result, 'trajectory') \\\n",
    "        else None\n",
    "\n",
    "    trace[trace_unit.drug] = trace_unit\n",
    "    \n",
    "    # Log after each step for validation\n",
    "    append_jsonl(log_file, record=trace_unit.model_dump())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy-litl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
